{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCNN GTSRB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/Colab_Notebooks/kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download meowmeowmeowmeowmeow/gtsrb-german-traffic-sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip gtsrb-german-traffic-sign.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTSRB Complete EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, random_split\n",
    "from torchvision import transforms, utils, datasets\n",
    "from google.colab import files\n",
    "from skimage import io, transform, color\n",
    "from trafficsignsdataset import GTSRBDataset\n",
    "from numpy.ma.core import MaskedConstant\n",
    "import torch.utils.data.dataloader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from morph2d import Morph2d\n",
    "from pseudoinversemod import pseudoInverse\n",
    "from tabulate import tabulate\n",
    "import datetime\n",
    "import scipy\n",
    "from torchmetrics.functional import roc\n",
    "from CIDeLong import delong_roc_variance\n",
    "from scipy import stats\n",
    "from MCNN import MCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"GTSRB EDA\"\n",
    "\n",
    "def eda_gtsrb(train_path, test_path):\n",
    "    # Read DataFrames\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    # Number of images\n",
    "    print('The train set contains {} images, and the test set contains {} images.\\\n",
    " We have a total of {} images'.format(len(train_df),\n",
    "                                      len(test_df),\n",
    "                                      len(train_df) + len(test_df)))\n",
    "    # Size of images\n",
    "    complete_w_df = pd.concat([train_df[\"Width\"], test_df[\"Width\"]], axis = 1)\n",
    "    complete_w_df.columns = [\"Train\", \"Test\"]\n",
    "\n",
    "    plt.figure(1)\n",
    "    complete_w_df.plot.hist(figsize=(10,6), color = [\"Gray\", \"Blue\"])\n",
    "    plt.title(\"Width of images in GTSRB dataset\", fontweight=\"bold\")\n",
    "    plt.xlabel(\"Pixels\")\n",
    "    plt.ylabel(\"No. of images\")\n",
    "    plt.show\n",
    "    #plt.savefig('/content/WidthHistCompleteDataset.png')\n",
    "\n",
    "    # Number of classes\n",
    "    complete_c_df = pd.concat([train_df[\"ClassId\"], test_df[\"ClassId\"]], axis = 1)\n",
    "    complete_c_df.columns = [\"Train\", \"Test\"]\n",
    "\n",
    "    plt.figure(2)\n",
    "    complete_c_df.plot.hist(bins = 43, figsize=(10,6), color = [\"Gray\", \"Blue\"])\n",
    "    plt.title(\"Classes in GTSRB dataset\", fontweight=\"bold\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"No. of images\")\n",
    "    plt.show\n",
    "    #plt.savefig('/content/ClassesHistCompleteDataset.png')\n",
    "\n",
    "    # Chosen classes\n",
    "    print(\"The classes chosen were: class 1 with {} training images and {} testing images,\\\n",
    "class 13 with {} training images and {} testing images\".format(len(train_df[train_df[\"ClassId\"]==1]),\n",
    "                                                 len(test_df[test_df[\"ClassId\"]==1]),\n",
    "                                                 len(train_df[train_df[\"ClassId\"]==13]),\n",
    "                                                 len(test_df[test_df[\"ClassId\"]==13])))\n",
    "    \n",
    "    # Size of images after filter by class\n",
    "    train_class1 = train_df[train_df[\"ClassId\"]==1]\n",
    "    train_class2 = train_df[train_df[\"ClassId\"]==2]\n",
    "    test_class1 = test_df[test_df[\"ClassId\"]==1]\n",
    "    test_class2 = test_df[test_df[\"ClassId\"]==2]\n",
    "\n",
    "    filtered_c_df = pd.concat([train_class1[\"Width\"],\n",
    "                               train_class2[\"Width\"],\n",
    "                               test_class1[\"Width\"],\n",
    "                               test_class2[\"Width\"]], axis = 1)\n",
    "    \n",
    "    filtered_c_df.columns = [\"Train Class 1\",\n",
    "                             \"Train Class 2\",\n",
    "                             \"Test Class 1\",\n",
    "                             \"Test Class 2\"]\n",
    "\n",
    "    plt.figure(3)\n",
    "    filtered_c_df.plot.hist(figsize=(10,6), color = [\"Black\", \"Gray\", \"Purple\", \"Blue\"])\n",
    "    plt.title(\"Width of images in GTSRB dataset for 2 classes\", fontweight=\"bold\")\n",
    "    plt.xlabel(\"Pixels\")\n",
    "    plt.ylabel(\"No. of images\")\n",
    "    plt.show\n",
    "    #plt.savefig('/content/WidthHist2ClassesDataset.png')\n",
    "\n",
    "\n",
    "train_path = '/content/Train.csv'\n",
    "test_path = '/content/Test.csv'\n",
    "\n",
    "eda_gtsrb(train_path, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterData(train_csv_file, test_csv_file, root_dir):\n",
    "    \"\"\"This function filters the GTSRB dataset. It takes images with a size\n",
    "    lower than 90 pixels and classes 1 and 13\"\"\"\n",
    "\n",
    "    # Variables definition\n",
    "    train_gtsrb = pd.read_csv(train_csv_file).sample(frac=1)\n",
    "    test_gtsrb = pd.read_csv(test_csv_file).sample(frac=1)\n",
    "    root_dir = root_dir\n",
    "    root_fname = '/content/FilteredData/'\n",
    "\n",
    "    # Folders creation\n",
    "    os.mkdir(\"/content/FilteredData\")\n",
    "    os.mkdir(\"/content/FilteredData/Train\")\n",
    "    os.mkdir(\"/content/FilteredData/Test\")\n",
    "    os.mkdir(\"/content/FilteredData/Train/1\")\n",
    "    os.mkdir(\"/content/FilteredData/Train/13\")\n",
    "\n",
    "    train_gtsrb = train_gtsrb.sample(frac=1)\n",
    "    test_gtsrb = test_gtsrb.sample(frac=1)\n",
    "    gtsrb = pd.concat([train_gtsrb, test_gtsrb], axis = 0)\n",
    "\n",
    "    train_gtsrb_new = pd.DataFrame(gtsrb.iloc[0,:], columns = train_gtsrb.columns)\n",
    "    test_gtsrb_new = pd.DataFrame(gtsrb.iloc[0,:], columns = test_gtsrb.columns)\n",
    "\n",
    "    # Filter data\n",
    "    for idx in range(len(train_gtsrb)):\n",
    "        if train_gtsrb.iloc[idx,0]<150:\n",
    "            if train_gtsrb.iloc[idx,6]==1 or train_gtsrb.iloc[idx,6]==13:\n",
    "                img_name = os.path.join(root_dir,\n",
    "                                    train_gtsrb.iloc[idx, 7])\n",
    "                image = io.imread(img_name)\n",
    "                train_gtsrb_new = train_gtsrb_new.append(train_gtsrb.iloc[idx,:],\n",
    "                                                         ignore_index=True)\n",
    "\n",
    "                fname = os.path.join(root_fname,\n",
    "                                    train_gtsrb.iloc[idx, 7])\n",
    "                io.imsave(fname, image)\n",
    "                train_gtsrb_new.to_csv('/content/FilteredData/train_gtsrb.csv')\n",
    "\n",
    "    for idx in range(len(test_gtsrb)):\n",
    "        if test_gtsrb.iloc[idx,0]<150: # 90\n",
    "            if test_gtsrb.iloc[idx,6]==1 or test_gtsrb.iloc[idx,6]==13:\n",
    "                img_name = os.path.join(root_dir,\n",
    "                                    test_gtsrb.iloc[idx, 7])\n",
    "                image = io.imread(img_name)\n",
    "                test_gtsrb_new = test_gtsrb_new.append(test_gtsrb.iloc[idx,:],\n",
    "                                                       ignore_index=True)\n",
    "\n",
    "                fname = os.path.join(root_fname,\n",
    "                                    test_gtsrb.iloc[idx, 7])\n",
    "                io.imsave(fname, image)\n",
    "                test_gtsrb_new.to_csv('/content/FilteredData/test_gtsrb.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterData(train_csv_file='/content/Train.csv', test_csv_file='/content/Test.csv', root_dir='/content/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Settings\n",
    "batch_size = 4037\n",
    "test_batch_size = 1730\n",
    "num_train_samples = 4037\n",
    "num_test_samples = 1730\n",
    "stop_train = num_train_samples/batch_size\n",
    "stop_test = num_test_samples/test_batch_size\n",
    "seed = 80\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "# Training settings\n",
    "traffic_train_dataset = GTSRBDataset(csv_file='/content/FilteredData/train_gtsrb.csv',\n",
    "                              root_dir='/content/FilteredData',\n",
    "                              transform=transforms.Compose([\n",
    "                                               Rescale(100),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(traffic_train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           worker_init_fn=seed_worker,\n",
    "                                           generator=g)\n",
    "\n",
    "# Testing settings\n",
    "traffic_test_dataset = GTSRBDataset(csv_file='/content/FilteredData/test_gtsrb.csv',\n",
    "                              root_dir='/content/FilteredData',\n",
    "                              transform=transforms.Compose([\n",
    "                                               Rescale(100),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(traffic_test_dataset,\n",
    "                                          batch_size=test_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          worker_init_fn=seed_worker,\n",
    "                                          generator=g)\n",
    "\n",
    "# Three definitions, each per channel, for weights' independance\n",
    "model_1 = MCNN()\n",
    "model_2 = MCNN()\n",
    "model_3 = MCNN()\n",
    "\n",
    "optimizer_1 = pseudoInverse(params=model_1.parameters(), C=1e-3)\n",
    "optimizer_2 = pseudoInverse(params=model_2.parameters(), C=1e-3)\n",
    "optimizer_3 = pseudoInverse(params=model_3.parameters(), C=1e-3)\n",
    "\n",
    "def plot_auc(fpr, tpr, auc_result):\n",
    "\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.plot(fpr, tpr, linewidth=1, color='blue')\n",
    "    plt.plot(np.linspace(0, max(fpr), len(fpr)),\n",
    "             np.linspace(0, max(tpr), len(tpr)),\n",
    "             \"--\", linewidth=0.8, color=\"black\")\n",
    "    plt.title(label=\"ROC-AUC. MCNN\",\n",
    "             fontsize=15,\n",
    "             fontweight=\"bold\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.legend([\"AUC: \" + str(round(auc_result.item(), 3))], loc='right')\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlim(right=max(tpr))\n",
    "    plt.ylim(top=max(fpr))\n",
    "\n",
    "def final_classifier(out_train_1, out_train_2, out_train_3, out_test_1,\n",
    "                     out_test_2, out_test_3, target_train, target_test):\n",
    "    \"\"\"This function performs a final classification considering the independent\n",
    "    DPFs obtained from the MCNN method.\n",
    "    \n",
    "    inputs:\n",
    "      out_trains = DPFs obtained per channel from the training set, it contains\n",
    "                   the probabilities of both classes [# classes, # subjects].\n",
    "      out_tests = DPFs obtained per channel from the testing set, it contains\n",
    "                  the probabilities of both classes [# classes, # subjects].\n",
    "      target_train = Labels corresponding to the subjects in the training set.\n",
    "      target_test = Labels corresponding to the subjects in the testing set.\"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Initial definitions\n",
    "    model1 = GaussianNB()\n",
    "    model2 = svm.SVC(random_state=seed, probability=True)\n",
    "    model3 = AdaBoostClassifier(random_state=seed)\n",
    "    model4 = RandomForestClassifier(random_state=seed)\n",
    "    model = [model1, model2, model3, model4]\n",
    "\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    acc_balanced = []\n",
    "    ci_bacc = []\n",
    "    ci_acc = []\n",
    "    conf_mat = []\n",
    "    auc = []\n",
    "    auc_delong = []\n",
    "    auc_cov = []\n",
    "    ci_auc = []\n",
    "    err = []\n",
    "    fpr_all = []\n",
    "    tpr_all = []\n",
    "    p_fpr_all = []\n",
    "    p_tpr_all = []\n",
    "\n",
    "    count = 1\n",
    "    alpha = 0.95\n",
    "    confidence = 0.95\n",
    "    z_value = stats.norm.ppf((1 + confidence) / 2.0)\n",
    "\n",
    "    acc_train.append(\"Train set accuracy\")\n",
    "    acc_test.append(\"Test set accuracy\")\n",
    "    acc_balanced.append(\"Balanced accuracy\")\n",
    "    ci_bacc.append(\"CI Balanced acc\")\n",
    "    ci_acc.append(\"CI Acc\")\n",
    "    auc.append(\"ROC AUC score\")\n",
    "    auc_delong.append(\"ROC AUC DeLong\")\n",
    "    ci_auc.append(\"CI AUC\")\n",
    "    auc_cov.append(\"AUC COV\")\n",
    "    err.append(\"Error\")\n",
    "    headers = [\"Metrics\", \"Gaussian Naive Bayes\", \"SVM\",\n",
    "                        \"AdaBoost\", \"Random Forest\"]\n",
    "\n",
    "    # Input data preparation\n",
    "    out_train_1 = out_train_1.detach().numpy()\n",
    "    out_train_2 = out_train_2.detach().numpy()\n",
    "    out_train_3 = out_train_3.detach().numpy()\n",
    "\n",
    "    joint_train = np.vstack((out_train_1[:,0],\n",
    "                             out_train_2[:,0],\n",
    "                             out_train_3[:,0]))\n",
    "    joint_train = np.transpose(joint_train)\n",
    "    joint_train = pd.DataFrame(joint_train)\n",
    "\n",
    "    out_test_1 = out_test_1.detach().numpy()\n",
    "    out_test_2 = out_test_2.detach().numpy()\n",
    "    out_test_3 = out_test_3.detach().numpy()\n",
    "\n",
    "    joint_test = np.vstack((out_test_1[:,0],\n",
    "                            out_test_2[:,0],\n",
    "                            out_test_3[:,0]))\n",
    "    joint_test = np.transpose(joint_test)\n",
    "    joint_test = pd.DataFrame(joint_test)\n",
    "\n",
    "    # Training\n",
    "    for mod in model:\n",
    "        trained_model = mod.fit(joint_train,target_train)\n",
    "        trained_model.fit(joint_train, target_train)\n",
    "        acc_train_ind = metrics.accuracy_score(target_train,\n",
    "                                               trained_model.predict(joint_train))\n",
    "        acc_train.append(acc_train_ind)\n",
    "\n",
    "    # Testing\n",
    "    for mod in model: \n",
    "        predicted = mod.predict(joint_test)\n",
    "        probs = mod.predict_proba(joint_test)\n",
    "        acc_test_ind = metrics.accuracy_score(target_test, predicted)\n",
    "        auc_ind = roc_auc_score(target_test, probs[:,1])\n",
    "        acc_balanced_ind = balanced_accuracy_score(target_test, predicted)\n",
    "        fpr, tpr, thresholds = roc_curve(target_test, probs[:,1], pos_label=1)\n",
    "        p_fpr, p_tpr, _ = roc_curve(target_test, predicted, pos_label=1)\n",
    "        err_ind = 1 - acc_test_ind\n",
    "\n",
    "        auc_ind_delong, auc_ind_cov = delong_roc_variance(target_test,\n",
    "                                                          probs[:,1])\n",
    "        \n",
    "        # AUC CIs\n",
    "        auc_std = np.sqrt(auc_ind_cov)\n",
    "        lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "        ci_auc_ind = stats.norm.ppf(\n",
    "            lower_upper_q,\n",
    "            loc=auc_ind_delong,\n",
    "            scale=auc_std)\n",
    "\n",
    "        ci_auc_ind[ci_auc_ind > 1] = 1\n",
    "        ci_auc_ind[0] = round(ci_auc_ind[0], 3)\n",
    "        ci_auc_ind[1] = round(ci_auc_ind[1], 3)\n",
    "\n",
    "        # Acc CIs\n",
    "        ci_length = z_value * np.sqrt((acc_test_ind * (1 - acc_test_ind)) / joint_test.shape[0])\n",
    "        ci_lower = acc_test_ind - ci_length\n",
    "        ci_upper = acc_test_ind + ci_length\n",
    "        ci_acc_ind = (round(ci_lower, 3), round(ci_upper, 3))\n",
    "\n",
    "        # Balanced acc CIs\n",
    "        ci_length = z_value * np.sqrt((acc_balanced_ind * (1 - acc_balanced_ind)) / joint_test.shape[0])\n",
    "        ci_lower = acc_balanced_ind - ci_length\n",
    "        ci_upper = acc_balanced_ind + ci_length\n",
    "        ci_bacc_ind = (round(ci_lower, 3), round(ci_upper, 3))\n",
    "\n",
    "        acc_test.append(round(acc_test_ind, 3))\n",
    "        ci_acc.append(ci_acc_ind)\n",
    "        acc_balanced.append(round(acc_balanced_ind, 3))\n",
    "        ci_bacc.append(ci_bacc_ind)\n",
    "        auc_delong.append(round(auc_ind_delong, 3))\n",
    "        auc_cov.append(round(auc_ind_cov, 3))\n",
    "        ci_auc.append(ci_auc_ind)\n",
    "        auc.append(round(auc_ind, 3))\n",
    "        err.append(round(err_ind, 3))\n",
    "        fpr_all.append(fpr)\n",
    "        tpr_all.append(tpr)\n",
    "        p_fpr_all.append(p_fpr)\n",
    "        p_tpr_all.append(p_tpr)\n",
    "\n",
    "        conf_mat_ind = confusion_matrix(target_test, predicted)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat_ind,\n",
    "                                      display_labels=mod.classes_)\n",
    "        disp.plot(cmap='Blues')\n",
    "        disp.ax_.set_title(headers[count])\n",
    "        count += 1\n",
    "        print(\"\\n\")\n",
    "\n",
    "        plot_auc(fpr, tpr, auc_ind)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Print metrics resulted\n",
    "    data = [acc_train, acc_test, ci_acc, acc_balanced, ci_bacc, auc_delong, ci_auc, auc_cov, err]\n",
    "    print(tabulate(data, headers))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return fpr_all, tpr_all, p_fpr_all, p_tpr_all\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"This functions performs the training process\"\"\"\n",
    "\n",
    "    print(\"Ensembles training process starts.........................\")\n",
    "    model_1.train()\n",
    "    model_2.train()\n",
    "    model_3.train()\n",
    "\n",
    "    for batch_idx, sample_batched in enumerate(train_loader):\n",
    "        if batch_idx > stop_train-1:\n",
    "          break\n",
    "        \n",
    "        # Input data and targets preparation\n",
    "        data = sample_batched['image'].float()\n",
    "        data_shape = data.shape\n",
    "        data_1 = torch.reshape(data[:,0,:,:],(data_shape[0],1,\n",
    "                                              data_shape[2], data_shape[3]))\n",
    "        data_2 = torch.reshape(data[:,1,:,:],(data_shape[0],1,\n",
    "                                              data_shape[2], data_shape[3]))\n",
    "        data_3 = torch.reshape(data[:,2,:,:],(data_shape[0],1,\n",
    "                                              data_shape[2], data_shape[3]))\n",
    "        target = sample_batched['label'].int()\n",
    "        target = target.reshape([len(target)])\n",
    "        data_1, data_2, data_3 = Variable(data_1), Variable(data_2), Variable(data_3)\n",
    "        target = Variable(target)\n",
    "\n",
    "        # Independent training per channel\n",
    "        hiddenOut = model_1.forwardToHidden(data_1) # First channel\n",
    "        optimizer_1.train(inputs=hiddenOut, targets=target)\n",
    "\n",
    "        hiddenOut = model_2.forwardToHidden(data_2) # Second channel\n",
    "        optimizer_2.train(inputs=hiddenOut, targets=target)\n",
    "\n",
    "        hiddenOut = model_3.forwardToHidden(data_3) # Third channel\n",
    "        optimizer_3.train(inputs=hiddenOut, targets=target)\n",
    "    \n",
    "    print(\"Ensembles training process ends...........................\")\n",
    "\n",
    "def train_accuracy():\n",
    "    \"\"\"This functions test the network using the training set.\n",
    "    \n",
    "       outputs:\n",
    "          output_1 = PDFs from first channel [# classes, # subjects]\n",
    "          output_2 = PDFs from second channel [# classes, # subjects]\n",
    "          output_3 = PDFs from third channel [# classes, # subjects]\n",
    "          target = labels of each subject\"\"\"\n",
    "\n",
    "    print(\"Ensembles training testing starts.........................\")\n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "    model_3.eval()\n",
    "\n",
    "    count_train = 0 # Temporal\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, sample_batched in enumerate(train_loader):\n",
    "            count_train = batch_idx\n",
    "            if batch_idx > stop_train-1:\n",
    "              break\n",
    "\n",
    "            # Input data and targets preparation\n",
    "            data = sample_batched['image'].float()\n",
    "            data_shape = data.shape\n",
    "            data_1 = torch.reshape(data[:,0,:,:],(data_shape[0],1,\n",
    "                                                  data_shape[2], data_shape[3]))\n",
    "            data_2 = torch.reshape(data[:,1,:,:],(data_shape[0],1,\n",
    "                                                  data_shape[2], data_shape[3]))\n",
    "            data_3 = torch.reshape(data[:,2,:,:],(data_shape[0],1,\n",
    "                                                  data_shape[2], data_shape[3]))\n",
    "            target = sample_batched['label'].int()\n",
    "            target = target.reshape([len(target)])\n",
    "            data_1, data_2, data_3 = Variable(data_1), Variable(data_2), Variable(data_3)\n",
    "            target = Variable(target)\n",
    "            output_1 = model_1.forward(data_1) # First channel\n",
    "            output_2 = model_2.forward(data_2) # Second channel\n",
    "            output_3 = model_3.forward(data_3) # Third channel\n",
    "\n",
    "    print(\"Ensembles training testing ends...........................\")\n",
    "    return output_1, output_2, output_3, target, count_train\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"This functions test the network using the testing set.\n",
    "    \n",
    "       outputs:\n",
    "          output_1 = PDFs from first channel [# classes, # subjects]\n",
    "          output_2 = PDFs from second channel [# classes, # subjects]\n",
    "          output_3 = PDFs from third channel [# classes, # subjects]\n",
    "          target = labels of each subject\"\"\"\n",
    "\n",
    "    print(\"Ensembles testing starts..................................\")\n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "    model_3.eval()\n",
    "    \n",
    "    count_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, sample_batched in enumerate(test_loader):\n",
    "            count_test = batch_idx\n",
    "            if batch_idx > stop_test-1:\n",
    "              break\n",
    "\n",
    "            # Input data and targets preparation\n",
    "            data = sample_batched['image'].float()\n",
    "            data_shape = data.shape\n",
    "            data_1 = torch.reshape(data[:,0,:,:],(data_shape[0],1,\n",
    "                                                  data_shape[2], data_shape[3]))\n",
    "            data_2 = torch.reshape(data[:,1,:,:],(data_shape[0],1,\n",
    "                                                  data_shape[2], data_shape[3]))\n",
    "            data_3 = torch.reshape(data[:,2,:,:],(data_shape[0],1,\n",
    "                                                  data_shape[2], data_shape[3]))\n",
    "            target = sample_batched['label'].int()\n",
    "            target = target.reshape([len(target)])\n",
    "            data_1, data_2, data_3 = Variable(data_1), Variable(data_2), Variable(data_3)\n",
    "            target = Variable(target)\n",
    "            output_1 = model_1.forward(data_1) # First channel\n",
    "            output_2 = model_2.forward(data_2) # Second channel\n",
    "            output_3 = model_3.forward(data_3) # Third channel\n",
    "        \n",
    "    return output_1, output_2, output_3, target, count_test\n",
    "\n",
    "# Complete MCNN\n",
    "init=time.time()\n",
    "train()\n",
    "train_time = time.time()\n",
    "out_train_1, out_train_2, out_train_3, target_train, count_train = train_accuracy()\n",
    "target_train[target_train==13]=0\n",
    "\n",
    "out_test_1, out_test_2, out_test_3, target_test, count_test = test()\n",
    "target_test[target_test==13]=0\n",
    "test_time = time.time()\n",
    "P_FPR, P_TPR, FPR, TPR = final_classifier(out_train_1, out_train_2, out_train_3,\n",
    "                 out_test_1, out_test_2, out_test_3,\n",
    "                 target_train, target_test)\n",
    "ending=time.time()\n",
    "\n",
    "print(\"Training time: \", str(datetime.timedelta(seconds = train_time - init)), \"\\n\",\n",
    "      \"Testing time: \", str(datetime.timedelta(seconds = test_time - train_time)), \"\\n\",\n",
    "      \"Final process time: \", str(datetime.timedelta(seconds = ending - test_time)), \"\\n\",\n",
    "      \"Complete process time: \", str(datetime.timedelta(seconds = ending - init)))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_accuracy(output, target):\n",
    "    correct = 0\n",
    "    pred=output.data.max(1)[1]\n",
    "    correct += pred.eq(target.data).cpu().sum()\n",
    "    print('\\nAccuracy: {}/{} ({:.2f}%)\\n'.format(correct,\n",
    "                                                 output.shape[0],\n",
    "                                                 100. * correct / output.shape[0]))\n",
    "    \n",
    "def ind_confusion_matrix(output_aux, target_aux):\n",
    "\n",
    "  confusion_matrix = torch.zeros([2,2])\n",
    "  TP = 0\n",
    "  TN = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "\n",
    "  def binarization(output_aux):\n",
    "    for indx in range(output_aux.shape[0]):\n",
    "      max_val = torch.max(output_aux[indx,:])\n",
    "      max_val -= 0.001\n",
    "      row = F.threshold(output_aux[indx,:], max_val.item(), 0, inplace=False)\n",
    "      row[row==torch.max(row).item()] = 1\n",
    "      output_aux[indx,:] = row\n",
    "    return output_aux\n",
    "\n",
    "  output_aux = binarization(output_aux)\n",
    "\n",
    "  for i in range(2):\n",
    "    confusion_matrix[i,:] = torch.bincount(target_aux[output_aux[:,i].nonzero()][:,0], minlength=2)\n",
    "\n",
    "  for i in range(2):\n",
    "    TP += confusion_matrix[i,i].item()\n",
    "    TN += torch.sum(confusion_matrix).item() - torch.sum(confusion_matrix[i,:]).item() - torch.sum(confusion_matrix[:,i]).item() + confusion_matrix[i,i].item()\n",
    "    FP +=  torch.sum(confusion_matrix[i,:]).item() - confusion_matrix[i,i].item()\n",
    "    FN +=  torch.sum(confusion_matrix[:,i]).item() - confusion_matrix[i,i].item()\n",
    "\n",
    "  print(\"Confusion Matrix: \\n\", confusion_matrix, \"\\n\")\n",
    "  print(\"True Positives: \", TP)\n",
    "  print(\"True Negatives: \", TN)\n",
    "  print(\"False Positive: \", FP)\n",
    "  print(\"False Negatives: \", FN)\n",
    "\n",
    "\n",
    "def metrics(output_aux, target_aux):\n",
    "\n",
    "  confusion_matrix = torch.zeros([14,14])\n",
    "  TP = torch.zeros([14])\n",
    "  TN = torch.zeros([14])\n",
    "  FP = torch.zeros([14])\n",
    "  FN = torch.zeros([14])\n",
    "\n",
    "  def binarization(output_aux):\n",
    "    for indx in range(output_aux.shape[0]):\n",
    "      max_val = torch.max(output_aux[indx,:])\n",
    "      max_val -= 0.001\n",
    "      row = F.threshold(output_aux[indx,:], max_val.item(), 0, inplace=False)\n",
    "      row[row==torch.max(row).item()] = 1\n",
    "      output_aux[indx,:] = row\n",
    "    return output_aux\n",
    "\n",
    "  output_aux = binarization(output_aux)\n",
    "\n",
    "  for i in range(14):\n",
    "    confusion_matrix[i,:] = torch.bincount(target_aux[output_aux[:,i].nonzero()][:,0], minlength=14)\n",
    "\n",
    "  for i in range(14):\n",
    "    TP[i] = confusion_matrix[i,i].item()\n",
    "    TN[i] = torch.sum(confusion_matrix).item() - torch.sum(confusion_matrix[i,:]).item() - torch.sum(confusion_matrix[:,i]).item() + confusion_matrix[i,i].item()\n",
    "    FP[i] =  torch.sum(confusion_matrix[i,:]).item() - confusion_matrix[i,i].item()\n",
    "    FN[i] =  torch.sum(confusion_matrix[:,i]).item() - confusion_matrix[i,i].item()\n",
    "\n",
    "  mean_TP = torch.mean(TP).item()\n",
    "  mean_TN = torch.mean(TN).item()\n",
    "  mean_FP = torch.mean(FP).item()\n",
    "  mean_FN = torch.mean(FN).item()\n",
    "\n",
    "  print(\"Average True Positives: \", mean_TP)\n",
    "  print(\"Average True Negatives: \", mean_TN)\n",
    "  print(\"Average False Positives: \", mean_FP)\n",
    "  print(\"Average False Negatives: \", mean_FN)\n",
    "\n",
    "\n",
    "print(\"Red Channel \\n\")\n",
    "print(\"Training:\")\n",
    "ind_accuracy(out_train_1, target_train)\n",
    "print(\"Testing:\")\n",
    "ind_accuracy(out_test_1, target_test)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Green Channel \\n\")\n",
    "print(\"Training:\")\n",
    "ind_accuracy(out_train_2, target_train)\n",
    "print(\"Testing:\")\n",
    "ind_accuracy(out_test_2, target_test)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Blue Channel \\n\")\n",
    "print(\"Training:\")\n",
    "ind_accuracy(out_train_3, target_train)\n",
    "print(\"Testing:\")\n",
    "ind_accuracy(out_test_3, target_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
