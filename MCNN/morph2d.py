# -*- coding: utf-8 -*-
"""Morph2d.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eJE0IdSLUWWYNNm0kVkgtozEnGubLDco
"""

import math
import pdb
import torch
import torch.nn as nn
import torch.nn.functional as F

class Morph2d(nn.Module):
    """ Custom morphological layer. Only for square kernel.
    It performs the following operations in a parallel form:
    - Dilation
    - Erosion
    - Sequence (opening, closing)
    - Subtraction (subtract any two pair of operations)
    
    PENDINGS:
    - It does not deal with subtraction of componsed operations yet.
    - The errors are printed but do not stop the operations
    - It does not deal with openings and closing at the same time."""
    
    def __init__(self, in_channels, 
                 out_channels, 
                 kernel_size, 
                 dilation=True, 
                 erosion=True, 
                 convolution=True, 
                 sequence= False, # ("dilation", "erosion")
                 subtraction= False # ("dilation", "erosion")
                 ):
        super(Morph2d, self).__init__()
        self.dilation = dilation
        self.erosion = erosion
        self.convolution = convolution
        self.sequence = sequence
        self.subtraction = subtraction
        self.subtraction = subtraction

        num_op = dilation + erosion + convolution + bool(sequence) + bool(subtraction)
        partial = out_channels/num_op

        if (partial - int(partial)) == 0:
          partial_out_channels = round(partial)
        else:
          print("ERROR: The number of out channels should be a multiple of the number of operations activated")

        if dilation:
          self.dil = Dilation2d(in_channels, partial_out_channels, kernel_size, soft_max=True, beta=20)
      
        if erosion:
          self.er = Erosion2d(in_channels, partial_out_channels, kernel_size, soft_max=True, beta=20)

        if convolution:
          self.conv = nn.Conv2d(in_channels, partial_out_channels, kernel_size, padding="same").double()

        if sequence:
          if sequence[0] == "dilation":
            self.dil_seq = Dilation2d(in_channels, partial_out_channels, kernel_size, soft_max=True, beta=20)
            self.er_seq = Erosion2d(partial_out_channels, partial_out_channels, kernel_size, soft_max=True, beta=20)
          elif sequence[0] == "erosion":
            self.dil_seq = Dilation2d(partial_out_channels, partial_out_channels, kernel_size, soft_max=True, beta=20)
            self.er_seq = Erosion2d(in_channels, partial_out_channels, kernel_size, soft_max=True, beta=20)
        
        if subtraction:
          components = []

          if "dilation" in subtraction:
            dil_sub = Dilation2d(in_channels, partial_out_channels, kernel_size, soft_max=True, beta=20)
        
          if "erosion" in subtraction:
            er_sub = Erosion2d(in_channels, partial_out_channels, kernel_size, soft_max=True, beta=20)


          if "original" in subtraction:
            if subtraction.index("original") == 0:
              components.append("original")
              if "dilation" in subtraction:
                components.append(dil_sub)
              elif "erosion" in subtraction:
                components.append(er_sub)
              else:
                print("ERROR: second term in subtraction is undefined")

            elif subtraction.index("original") == 1:
              if "dilation" in subtraction:
                components.append(dil_sub)
              elif "erosion" in subtraction:
                components.append(er_sub)
              else:
                print("ERROR: first term in subtraction is undefined")
              components.append("original")

          else:
            if subtraction.index("dilation") == 0 and subtraction.index("erosion") == 1:
              components.append(dil_sub)
              components.append(er_sub)
            elif subtraction.index("erosion") == 0 and subtraction.index("dilation") == 1:
              components.append(er_sub)
              components.append(dil_sub)
            else:
              print("ERROR: undefined terms in subtraction")
          
          self.subtraction_components = components

    def forward(self, x):

      initial_out = torch.Tensor([])

      if self.dilation:
        dil_out = self.dil(x)
        out = torch.cat((initial_out, dil_out),1)
      
      if self.erosion:
        er_out = self.er(x)
        if out.shape[0] == 0:
          out = torch.cat((initial_out, er_out),1)
        else:
          out = torch.cat((out, er_out),1)

      if self.convolution:
        conv_out = self.conv(x.double())
        if out.shape[0] == 0:
          out = torch.cat((initial_out, conv_out),1)
        else:
          out = torch.cat((out, conv_out),1)

      if self.sequence:
        if self.sequence[0] == "dilation" and self.sequence[1] == "erosion":
          seq_out = self.dil_seq(x)
          seq_out = self.er_seq(seq_out)
        elif self.sequence[0] == "erosion" and self.sequence[1] == "dilation":
          seq_out = self.er_seq(x)
          seq_out = self.dil_seq(seq_out)

        if out.shape[0] == 0:
          out = torch.cat((initial_out, seq_out),1)
        else:
          out = torch.cat((out, seq_out),1)
      
      if self.subtraction:
        if self.subtraction_components[0] == "original":
          sub_out1 = x
          sub_out2 = self.subtraction_components[1](x)
        elif self.subtraction_components[1] == "original":
          sub_out1 = self.subtraction_components[0](x)
          sub_out2 = x
        else:
          sub_out1 = self.subtraction_components[0](x)
          sub_out2 = self.subtraction_components[1](x)
        sub_out = sub_out1 - sub_out2

        if out.shape[0] == 0:
          out = torch.cat((initial_out, sub_out),1)
        else:
          out = torch.cat((out, sub_out),1)
      
      return out

class Morphology(nn.Module):
    '''
    Base class for morpholigical operators 
    For now, only supports stride=1, dilation=1, kernel_size H==W, and padding='same'.
    '''
    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=15, type=None):
        '''
        in_channels: scalar
        out_channels: scalar, the number of the morphological neure. 
        kernel_size: scalar, the spatial size of the morphological neure.
        soft_max: bool, using the soft max rather the torch.max(), ref: Dense Morphological Networks: An Universal Function Approximator (Mondal et al. (2019)).
        beta: scalar, used by soft_max.
        type: str, dilation2d or erosion2d.
        '''
        super(Morphology, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.soft_max = soft_max
        self.beta = beta
        self.type = type

        self.weight = nn.Parameter(torch.zeros(out_channels, in_channels, kernel_size, kernel_size), requires_grad=True)
        self.unfold = nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)

    def forward(self, x):
        '''
        x: tensor of shape (B,C,H,W)
        '''
        # padding
        x = fixed_padding(x, self.kernel_size, dilation=1)
        
        # unfold
        x = self.unfold(x)  # (B, Cin*kH*kW, L), where L is the numbers of patches
        x = x.unsqueeze(1)  # (B, 1, Cin*kH*kW, L)
        L = x.size(-1)
        L_sqrt = int(math.sqrt(L))

        # erosion
        weight = self.weight.view(self.out_channels, -1) # (Cout, Cin*kH*kW)
        weight = weight.unsqueeze(0).unsqueeze(-1)  # (1, Cout, Cin*kH*kW, 1)

        if self.type == 'erosion2d':
            x = weight - x # (B, Cout, Cin*kH*kW, L)
        elif self.type == 'dilation2d':
            x = weight + x # (B, Cout, Cin*kH*kW, L)
        else:
            raise ValueError
        
        if not self.soft_max:
            x, _ = torch.max(x, dim=2, keepdim=False) # (B, Cout, L)
        else:
            x = torch.logsumexp(x*self.beta, dim=2, keepdim=False) / self.beta # (B, Cout, L)

        if self.type == 'erosion2d':
            x = -1 * x

        # instead of fold, we use view to avoid copy
        x = x.view(-1, self.out_channels, L_sqrt, L_sqrt)  # (B, Cout, L/2, L/2)

        return x 

class Dilation2d(Morphology):
    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):
        super(Dilation2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'dilation2d')

class Erosion2d(Morphology):
    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):
        super(Erosion2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'erosion2d')



def fixed_padding(inputs, kernel_size, dilation):
    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)
    pad_total = kernel_size_effective - 1
    pad_beg = pad_total // 2
    pad_end = pad_total - pad_beg
    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))
    return padded_inputs
    
    
if __name__ == '__main__':
    # test
    x=torch.randn(2,3,16,16)
    e=Erosion2d(3, 4, 3, soft_max=False)
    y=e(x)